{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CMPSC310-AlleghenyCollege-Fall2021/activity18/blob/main/NLP_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWoVM8x1Ph8H"
      },
      "source": [
        "# Activity 18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfKt9ernTGcK"
      },
      "source": [
        "## Setting up the Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH21OrjWooPS"
      },
      "source": [
        "!pip3 install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5aFgDKqOBI"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4JpHHRXTKrI"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PwgHiH5qTQs"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZAtuBOHr0Hi"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "#TODO: tokenize text by words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZavEyf3shdA"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "#TODO: tokenize text by sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug23r1ziTRBB"
      },
      "source": [
        "## Stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J_X7WZCtvSO"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkFPhwmAt0_m"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words(\"english\")\n",
        "print(stopwords)\n",
        "#TODO: remove stopwords from text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPDmzpqRTUdn"
      },
      "source": [
        "## Stemming Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st2JbM81u0RJ",
        "outputId": "b7c145b7-7e74-43b9-ba44-00605addf2fe"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "words = [\"Loving\", \"Chocolate\", \"Retrieved\", \"Being\"] # TODO: modify words and observe new outcome\n",
        "for i in words:\n",
        "   print(ps.stem(i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "love\n",
            "chocol\n",
            "retriev\n",
            "be\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBjGg2nrTYAm"
      },
      "source": [
        "## Counting Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WchynDMVnu8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9584595a-d6e3-4712-a979-1556ecd896cb"
      },
      "source": [
        "import nltk\n",
        "words = [\"flower\", \"sun\", \"flower\", \"rain\"] # TODO: modify words and observe new outcome\n",
        "FreqDist = nltk.FreqDist(words)\n",
        "for i,j in FreqDist.items():\n",
        "   print(i, \"--\", j)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flower -- 2\n",
            "sun -- 1\n",
            "rain -- 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6au6fb1Tilk"
      },
      "source": [
        "## Word groups\n",
        "\n",
        "Run three examples below and observe the differences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eWR25NzQDtv",
        "outputId": "448ff98b-dce8-458d-e3bd-de6d1b5ded0a"
      },
      "source": [
        "words = \"All happy families are alike; each unhappy family is unhappy in its own way.\"\n",
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(list(nltk.bigrams(word_tokenize)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('All', 'happy'), ('happy', 'families'), ('families', 'are'), ('are', 'alike'), ('alike', ';'), (';', 'each'), ('each', 'unhappy'), ('unhappy', 'family'), ('family', 'is'), ('is', 'unhappy'), ('unhappy', 'in'), ('in', 'its'), ('its', 'own'), ('own', 'way'), ('way', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzrbKs2jRFfP",
        "outputId": "e3173d84-af45-4a41-a89b-47e52571a3c0"
      },
      "source": [
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(list(nltk.trigrams(word_tokenize)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('All', 'happy', 'families'), ('happy', 'families', 'are'), ('families', 'are', 'alike'), ('are', 'alike', ';'), ('alike', ';', 'each'), (';', 'each', 'unhappy'), ('each', 'unhappy', 'family'), ('unhappy', 'family', 'is'), ('family', 'is', 'unhappy'), ('is', 'unhappy', 'in'), ('unhappy', 'in', 'its'), ('in', 'its', 'own'), ('its', 'own', 'way'), ('own', 'way', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj1BCGX_RX_J",
        "outputId": "4afe9101-a535-43a9-e18c-5c4495f39e92"
      },
      "source": [
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(list(nltk.ngrams(word_tokenize, 4)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('All', 'happy', 'families', 'are'), ('happy', 'families', 'are', 'alike'), ('families', 'are', 'alike', ';'), ('are', 'alike', ';', 'each'), ('alike', ';', 'each', 'unhappy'), (';', 'each', 'unhappy', 'family'), ('each', 'unhappy', 'family', 'is'), ('unhappy', 'family', 'is', 'unhappy'), ('family', 'is', 'unhappy', 'in'), ('is', 'unhappy', 'in', 'its'), ('unhappy', 'in', 'its', 'own'), ('in', 'its', 'own', 'way'), ('its', 'own', 'way', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7BwByxESK4n"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "Experiment with lemmatizer below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3mvsterSH2a",
        "outputId": "286bc925-dea3-42be-d3b5-edc8757fa17f"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZiPb5cPSRAG",
        "outputId": "ba208ab2-e6bf-4f2b-d583-fd0c298e2d20"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lem = WordNetLemmatizer()\n",
        "print(lem.lemmatize(\"believes\"))\n",
        "print(lem.lemmatize(\"stripes\"))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "belief\n",
            "stripe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lgt9YaYISxMv",
        "outputId": "cefe38d8-b718-4659-971d-2a192d69997e"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "em = WordNetLemmatizer()\n",
        "print(lem.lemmatize(\"believes\", pos=\"v\"))\n",
        "print(em.lemmatize(\"stripes\", pos=\"v\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "believe\n",
            "strip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd30v8wvS8cM"
      },
      "source": [
        "## POS Taggers\n",
        "\n",
        "[POS Tags](https://drive.google.com/file/d/146BDCH-yYmiMxMCR9iJbkn3JqxbU97Bz/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_Z59DxgTCaD",
        "outputId": "70987eb7-e716-4f35-a6ef-18a03b72986c"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "words = \"All happy families are alike; each unhappy family is unhappy in its own way.\" #TODO: experiment with other text\n",
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(nltk.pos_tag(word_tokenize))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[('All', 'DT'), ('happy', 'JJ'), ('families', 'NNS'), ('are', 'VBP'), ('alike', 'RB'), (';', ':'), ('each', 'DT'), ('unhappy', 'JJ'), ('family', 'NN'), ('is', 'VBZ'), ('unhappy', 'JJ'), ('in', 'IN'), ('its', 'PRP$'), ('own', 'JJ'), ('way', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vGnMzBTVAn6"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hAhTQJaU--_",
        "outputId": "d8864dfb-351d-4548-9b81-c01b6729b598"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1NpKVSHWSfj",
        "outputId": "be7765b2-a7ee-4629-8821-422e3865df6a"
      },
      "source": [
        "text = \"The American president Joe Biden is in the White House\" #TODO: experiment with other text\n",
        "tokenize = nltk.word_tokenize(text)\n",
        "POS_tags = nltk.pos_tag(tokenize)\n",
        "nameEn = nltk.ne_chunk(POS_tags)\n",
        "print(nameEn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  The/DT\n",
            "  (GPE American/NNP)\n",
            "  president/NN\n",
            "  (PERSON Joe/NNP Biden/NNP)\n",
            "  is/VBZ\n",
            "  in/IN\n",
            "  the/DT\n",
            "  (FACILITY White/NNP House/NNP))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE8Qr4LdYYzJ"
      },
      "source": [
        "## Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekKMimU1YZ1B",
        "outputId": "4b1d36c8-f4d7-470b-8d07-816e5b20c5d4"
      },
      "source": [
        "!pip3 install textblob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5DGtIvNYffo",
        "outputId": "8f6588cd-f3d8-4c66-f2a3-69857089cd9e"
      },
      "source": [
        "from textblob import TextBlob\n",
        "Joe_Biden_Tweet = \"Small businesses need relief, but many were muscled out of the way by big companies last year.\" #TODO: change the text and re-run. How did polarity and subectivity change?\n",
        "Joe_Biden = TextBlob(Joe_Biden_Tweet)\n",
        "print(Joe_Biden.sentiment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment(polarity=0.0625, subjectivity=0.26666666666666666)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG2gqbhMZqP8"
      },
      "source": [
        "## Spelling Correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9QWtSAyZqvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced254b8-f4b9-49ff-86ff-59a97c41cade"
      },
      "source": [
        "from textblob import TextBlob\n",
        "Text = \"Smalle businesses neede relief\" #TODO: Experiment with another text\n",
        "spelling_mistakes = TextBlob(Text)\n",
        "print(spelling_mistakes.correct())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Small business need relief\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}