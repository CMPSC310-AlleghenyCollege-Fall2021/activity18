{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CMPSC310-AlleghenyCollege-Fall2021/activity18/blob/main/NLP_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWoVM8x1Ph8H"
      },
      "source": [
        "# Activity 18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfKt9ernTGcK"
      },
      "source": [
        "## Setting up the Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH21OrjWooPS"
      },
      "source": [
        "!pip3 install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5aFgDKqOBI"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4JpHHRXTKrI"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PwgHiH5qTQs"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZAtuBOHr0Hi"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "#TODO: tokenize text by words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZavEyf3shdA"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "#TODO: tokenize text by sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug23r1ziTRBB"
      },
      "source": [
        "## Stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J_X7WZCtvSO"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkFPhwmAt0_m"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words(\"english\")\n",
        "print(stopwords)\n",
        "#TODO: remove stopwords from text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPDmzpqRTUdn"
      },
      "source": [
        "## Stemming Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st2JbM81u0RJ"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "words = [\"Loving\", \"Chocolate\", \"Retrieved\", \"Being\"] # TODO: modify words and observe new outcome\n",
        "for i in words:\n",
        "   print(ps.stem(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBjGg2nrTYAm"
      },
      "source": [
        "## Counting Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WchynDMVnu8"
      },
      "source": [
        "import nltk\n",
        "words = [\"flower\", \"sun\", \"flower\", \"rain\"] # TODO: modify words and observe new outcome\n",
        "FreqDist = nltk.FreqDist(words)\n",
        "for i,j in FreqDist.items():\n",
        "   print(i, \"--\", j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6au6fb1Tilk"
      },
      "source": [
        "## Word groups\n",
        "\n",
        "Run three examples below and observe the differences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eWR25NzQDtv"
      },
      "source": [
        "words = \"All happy families are alike; each unhappy family is unhappy in its own way.\"\n",
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(list(nltk.bigrams(word_tokenize)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzrbKs2jRFfP"
      },
      "source": [
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(list(nltk.trigrams(word_tokenize)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj1BCGX_RX_J"
      },
      "source": [
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(list(nltk.ngrams(word_tokenize, 4)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7BwByxESK4n"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "Experiment with lemmatizer below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3mvsterSH2a"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZiPb5cPSRAG"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lem = WordNetLemmatizer()\n",
        "print(lem.lemmatize(\"believes\"))\n",
        "print(lem.lemmatize(\"stripes\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgt9YaYISxMv"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "em = WordNetLemmatizer()\n",
        "print(lem.lemmatize(\"believes\", pos=\"v\"))\n",
        "print(em.lemmatize(\"stripes\", pos=\"v\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd30v8wvS8cM"
      },
      "source": [
        "## POS Taggers\n",
        "\n",
        "[POS Tags](https://drive.google.com/file/d/146BDCH-yYmiMxMCR9iJbkn3JqxbU97Bz/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Z59DxgTCaD"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "words = \"All happy families are alike; each unhappy family is unhappy in its own way.\" #TODO: experiment with other text\n",
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(nltk.pos_tag(word_tokenize))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vGnMzBTVAn6"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hAhTQJaU--_"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1NpKVSHWSfj"
      },
      "source": [
        "text = \"The American president Joe Biden is in the White House\" #TODO: experiment with other text\n",
        "tokenize = nltk.word_tokenize(text)\n",
        "POS_tags = nltk.pos_tag(tokenize)\n",
        "nameEn = nltk.ne_chunk(POS_tags)\n",
        "print(nameEn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE8Qr4LdYYzJ"
      },
      "source": [
        "## Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekKMimU1YZ1B"
      },
      "source": [
        "!pip3 install textblob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5DGtIvNYffo"
      },
      "source": [
        "from textblob import TextBlob\n",
        "Joe_Biden_Tweet = \"Small businesses need relief, but many were muscled out of the way by big companies last year.\" #TODO: change the text and re-run. How did polarity and subectivity change?\n",
        "Joe_Biden = TextBlob(Joe_Biden_Tweet)\n",
        "print(Joe_Biden.sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG2gqbhMZqP8"
      },
      "source": [
        "## Spelling Correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9QWtSAyZqvO"
      },
      "source": [
        "from textblob import TextBlob\n",
        "Text = \"Smalle businesses neede relief\" #TODO: Experiment with another text\n",
        "spelling_mistakes = TextBlob(Text)\n",
        "print(spelling_mistakes.correct())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}